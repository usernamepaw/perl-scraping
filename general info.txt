Overview:

The script will go to bloomberg.com and then capture the articles on this main page. When you look at the page you will see that the Title of the article is in bold and above it is the 'category' or classification of that article. SOme of the more common are; Markets, Travel, Lega, Wealth, and so on. The script captures this information as well as the text and images.

The data is saved in a csv file where the vertical bar '|' acts as a field separator. These are the fields saved:

Publication Date|Category|Headline|Author 1|Author 2|Author 3|Author 4|Description|Download Date|URL|Image 1 Name|Image 1 Source|Image 1 Title|Image 2 Name|Image 2 Source|Image 2 Title|Image 3 Name|Image 3 Source|Image 3 Title|Image 4 Name|Image 4 Source|Image 4 Title|Image 5 Name|Image 5 Source|Image 5 Title|Image 6 Name|Image 6 Source|Image 6 Title|Image 7 Name|Image 7 Source|Image 7 Title|Image 8 Name|Image 8 Source|Image 8 Title|Image 9 Name|Image 9 Source|Image 9 Title|Image 10 Name|Image 10 Source|Image 10 Title|Image 11 Name|Image 11 Source|Image 11 Title|Image 12 Name|Image 12 Source|Image 12 Title|Image 13 Name|Image 13 Source|Image 13 Title|Image 14 Name|Image 14 Source|Image 14 Title|Image 15 Name|Image 15 Source|Image 15 Title|Image 16 Name|Image 16 Source|Image 16 Title|Image 17 Name|Image 17 Source|Image 17 Title|Image 18 Name|Image 18 Source|Image 18 Title|Image 19 Name|Image 19 Source|Image 19 Title|Image 20 Name|Image 20 Source|Image 20 Title

As you can see we capture up to 20 images, 4 authors, and various fields.

At this time the page formating is saved so there are paragraphs in the csv file. This makes it easy to load into a text editor (I use UltraEdit) and then read the article. There is a problem with this as I also want to import this data into a MySQL database so I need to add a line terminator when loading into MySQL. 

The operation of the script is basically like this.

1. The program reads in the command line typically:

$ perl bloomberg_scraper_v6c.pl --output_file /media/john/DATA/a-News/news_Bloomberg/Current/test_6c_15minute.csv

Where the only option is the exported file name.

In an attempt to prevent Bloomberg from realizing that I am using a scraper we have set some options such as

2. The minimum and maximum seconds between following each link.
3. If the regex or module can not parse the data and throws an error we wait some time before retrying the url.
4. After getting all of the links on the main page we again wait (typically betweem 15 to 25 minutes before scraping again.


The script also has the ability to read in a text file of URL's and get the relevant data.

In this attachment you can see a sample of the data.

---------------------------------------------------------------

The program was working with no problems for almost a month. Four days ago it stopped. I thought it might be that I was banned so I downloaded a VPN (Private Internet Access or PIA) and the program still refused to work. PLease understand that I have an account wit Bloomberg which I use every day so I doubted it was a IP ban.

I am running the script on Ubuntu 22.04, and as the articles were downloaded I could see the date/time and url of the article. If it was of interest I can select the url within the terminal window and open the article in a browser (Brave). In any case using the VPN did not help. I think that there is a problem in the Jason module. I have also included the message displayed in the terminal when this occured 'error - jason error.txt'.

The programmer who wrote this was working on adding the abilty to safe each article as a pdf file. Two weeks ago he said he was almost done. When the script stopped working I sent him a message and there was no reply. Today I sent him a message telling him that I was posting this job to fix the code unless he replied. He did, telling me that he got a job in a office and no longer had time to fix the code or finish the pdf option.

So, what I want from you is what I mentioned in the posting:

1. Please get the program working again.
2. Please comment the code as you are reading it. Note, all comments are mine.
3. Please add the abiltity to save the downloaded data to two files:
	1) Have all of the data saved to the csv file on one line so it is easily loaded into a MySQL DB.
	2) Continue to use the existing format.
4. If the connection is being blocked then print to the terminal something to that affect like, "no data downloaded".
5. A flag within the code to save all of the raw data downloaded.
6. I would like the option to sign in to my account with my user ID and PW>
7. If a regex or module fails a message prints to the command window with the line number of the script which has the regex.

If you have any questions please ask. Thank you for your help.

John


